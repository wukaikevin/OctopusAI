# Copyright (c) 2024 Bytedance Ltd. and/or its affiliates
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import os
import time
import psutil
import gc
from contextlib import contextmanager
from omegaconf import OmegaConf
import torch
import torch.nn.functional as F
from diffusers import AutoencoderKL, DDIMScheduler
from latentsync.models.unet import UNet3DConditionModel
from latentsync.pipelines.lipsync_pipeline import LipsyncPipeline
from accelerate.utils import set_seed
from latentsync.whisper.audio2feature import Audio2Feature
from DeepCache import DeepCacheSDHelper
import xformers
import xformers.ops


@contextmanager
def torch_gc():
    """æ¸…ç†GPUå†…å­˜çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    try:
        yield
    finally:
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()


class MemoryOptimizer:
    """å†…å­˜ä¼˜åŒ–å™¨"""
    
    @staticmethod
    def optimize_pytorch_memory():
        """ä¼˜åŒ–PyTorchå†…å­˜è®¾ç½®"""
        if torch.cuda.is_available():
            # è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
            # å¯ç”¨TF32ä»¥åŠ é€Ÿè®¡ç®—
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            # å¯ç”¨cudnnåŸºå‡†æµ‹è¯•
            torch.backends.cudnn.benchmark = True
            # è®¾ç½®å†…å­˜ç¢ç‰‡æ•´ç†
            torch.cuda.set_per_process_memory_fraction(0.9)  # é¢„ç•™10%å†…å­˜
    
    @staticmethod
    def get_gpu_memory_info():
        """è·å–GPUå†…å­˜ä¿¡æ¯"""
        if torch.cuda.is_available():
            return {
                'allocated': torch.cuda.memory_allocated() / 1024**3,
                'cached': torch.cuda.memory_reserved() / 1024**3,
                'total': torch.cuda.get_device_properties(0).total_memory / 1024**3
            }
        return {}
    
    @staticmethod
    def clear_memory():
        """æ¸…ç†å†…å­˜"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()


class VideoProcessor:
    """è§†é¢‘å¤„ç†ä¼˜åŒ–å™¨"""
    
    @staticmethod
    def estimate_video_length(video_path):
        """ä¼°è®¡è§†é¢‘é•¿åº¦"""
        try:
            # ä½¿ç”¨ffprobeè·å–è§†é¢‘æ—¶é•¿
            import subprocess
            cmd = [
                'ffprobe', '-v', 'error',
                '-select_streams', 'v:0',
                '-show_entries', 'stream=duration',
                '-of', 'default=noprint_wrappers=1:nokey=1',
                video_path
            ]
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                return float(result.stdout.strip())
        except:
            pass
        return 60  # é»˜è®¤1åˆ†é’Ÿ


class OptimizedLipsyncPipeline(LipsyncPipeline):
    """ä¼˜åŒ–çš„å”‡å½¢åŒæ­¥Pipeline"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.enable_xformers = kwargs.get('enable_xformers', True)
        self.use_chunked_processing = kwargs.get('use_chunked_processing', True)
        self.chunk_size = kwargs.get('chunk_size', 16)
        
        if self.enable_xformers and hasattr(self.unet, 'set_use_memory_efficient_attention_xformers'):
            self.unet.set_use_memory_efficient_attention_xformers(True)
    
    def process_in_chunks(self, latents, audio_features, guidance_scale, num_inference_steps):
        """åˆ†å—å¤„ç†ä»¥å‡å°‘å†…å­˜ä½¿ç”¨"""
        if not self.use_chunked_processing or latents.shape[0] <= self.chunk_size:
            return self._process_whole(latents, audio_features, guidance_scale, num_inference_steps)
        
        results = []
        num_chunks = (latents.shape[0] + self.chunk_size - 1) // self.chunk_size
        
        for i in range(num_chunks):
            start_idx = i * self.chunk_size
            end_idx = min((i + 1) * self.chunk_size, latents.shape[0])
            
            chunk_latents = latents[start_idx:end_idx]
            chunk_audio = audio_features[start_idx:end_idx]
            
            with torch_gc():
                processed_chunk = self._process_whole(
                    chunk_latents, chunk_audio, guidance_scale, num_inference_steps
                )
            
            results.append(processed_chunk)
            
            # é‡Šæ”¾ä¸­é—´å˜é‡å†…å­˜
            del chunk_latents, chunk_audio
            if i < num_chunks - 1:  # ä¿ç•™æœ€åä¸€ä¸ªchunkçš„æ¢¯åº¦
                processed_chunk = processed_chunk.detach()
        
        return torch.cat(results, dim=0)
    
    def _process_whole(self, latents, audio_features, guidance_scale, num_inference_steps):
        """åŸæœ‰çš„å®Œæ•´å¤„ç†é€»è¾‘"""
        # è¿™é‡Œè°ƒç”¨çˆ¶ç±»çš„å¤„ç†æ–¹æ³•
        return super().forward(
            latents=latents,
            audio_features=audio_features,
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps
        )


def main(config, args):
    """ä¸»å‡½æ•° - ä¼˜åŒ–çš„æ¨ç†æµç¨‹"""
    if not os.path.exists(args.video_path):
        raise RuntimeError(f"Video path '{args.video_path}' not found")
    if not os.path.exists(args.audio_path):
        raise RuntimeError(f"Audio path '{args.audio_path}' not found")
    
    print(f"ğŸš€ Starting optimized inference")
    print(f"ğŸ“¹ Input video: {args.video_path}")
    print(f"ğŸµ Input audio: {args.audio_path}")
    
    start_time = time.time()
    
    # ä¼˜åŒ–å†…å­˜è®¾ç½®
    MemoryOptimizer.optimize_pytorch_memory()
    
    # æ£€æŸ¥GPUèƒ½åŠ›å¹¶é€‰æ‹©æœ€ä½³ç²¾åº¦
    if torch.cuda.is_available():
        compute_capability = torch.cuda.get_device_capability()
        gpu_name = torch.cuda.get_device_name(0)
        
        print(f"ğŸ® GPU: {gpu_name}")
        print(f"ğŸ”§ Compute Capability: {compute_capability}")
        
        # ä¼˜å…ˆé€‰æ‹©æ›´å¿«çš„ç²¾åº¦
        if compute_capability[0] >= 8:  # AmpereåŠä»¥ä¸Šæ¶æ„
            dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
            print("âœ… Using bfloat16 precision (fastest)")
        elif compute_capability[0] >= 7:  # VoltaåŠä»¥ä¸Šæ¶æ„
            dtype = torch.float16
            print("âœ… Using float16 precision")
        else:
            dtype = torch.float32
            print("âš ï¸ Using float32 precision (GPU may be slow)")
    else:
        dtype = torch.float32
        print("âš ï¸ Using CPU with float32 precision")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åŠ è½½é…ç½®
    print(f"ğŸ“„ Loaded checkpoint: {args.inference_ckpt_path}")
    
    # ä½¿ç”¨æ›´å¿«çš„scheduler
    scheduler = DDIMScheduler.from_pretrained(
        "configs",
        prediction_type="epsilon",
        timestep_spacing="trailing",  # æ›´å¿«çš„é‡‡æ ·
        clip_sample=False,
        set_alpha_to_one=False,
        steps_offset=1,
    )
    
    # åŠ¨æ€é€‰æ‹©whisperæ¨¡å‹
    if config.model.cross_attention_dim == 768:
        whisper_model_path = "checkpoints/whisper/small.pt"
        print("ğŸ”Š Using Whisper small model")
    elif config.model.cross_attention_dim == 384:
        whisper_model_path = "checkpoints/whisper/tiny.pt"
        print("ğŸ”Š Using Whisper tiny model (faster)")
    else:
        raise NotImplementedError("cross_attention_dim must be 768 or 384")
    
    # åŠ è½½éŸ³é¢‘ç¼–ç å™¨
    print("ğŸµ Loading audio encoder...")
    audio_encoder = Audio2Feature(
        model_path=whisper_model_path,
        device="cuda" if torch.cuda.is_available() else "cpu",
        num_frames=config.data.num_frames,
        audio_feat_length=config.data.audio_feat_length,
    )
    
    # åŠ è½½VAE - ä½¿ç”¨æ›´å¿«çš„è®¾ç½®
    print("ğŸ¨ Loading VAE...")
    vae = AutoencoderKL.from_pretrained(
        "stabilityai/sd-vae-ft-mse",
        torch_dtype=dtype,
        use_safetensors=True,
    )
    vae.config.scaling_factor = 0.18215
    vae.config.shift_factor = 0
    
    # å¯ç”¨VAE tilingä»¥èŠ‚çœå†…å­˜
    if hasattr(vae, 'enable_tiling'):
        vae.enable_tiling()
        print("âœ… Enabled VAE tiling for memory efficiency")
    
    # åŠ è½½UNet - ä¼˜åŒ–åŠ è½½è¿‡ç¨‹
    print("ğŸ§  Loading UNet...")
    unet, _ = UNet3DConditionModel.from_pretrained(
        OmegaConf.to_container(config.model),
        args.inference_ckpt_path,
        device="cpu",
        torch_dtype=dtype,
        use_safetensors=True if args.inference_ckpt_path.endswith('.safetensors') else False,
    )
    
    # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
    print(f"ğŸšš Moving models to {device}...")
    vae = vae.to(device=device, dtype=dtype)
    unet = unet.to(device=device, dtype=dtype)
    
    # ç¼–è¯‘æ¨¡å‹åŠ é€Ÿï¼ˆPyTorch 2.0+ï¼‰
    if torch.__version__ >= "2.0" and args.compile_model:
        try:
            print("âš¡ Compiling models with torch.compile...")
            # ç¼–è¯‘ä¸»è¦æ¨¡å‹
            unet = torch.compile(
                unet,
                mode="reduce-overhead",
                fullgraph=False,
                dynamic=False
            )
            vae = torch.compile(
                vae,
                mode="reduce-overhead",
                fullgraph=False,
                dynamic=False
            )
            print("âœ… Models compiled successfully")
        except Exception as e:
            print(f"âš ï¸ Model compilation failed: {e}. Continuing without compilation.")
    
    # åˆ›å»ºä¼˜åŒ–çš„pipeline
    print("ğŸ”§ Creating optimized pipeline...")
    pipeline = OptimizedLipsyncPipeline(
        vae=vae,
        audio_encoder=audio_encoder,
        unet=unet,
        scheduler=scheduler,
        enable_xformers=args.enable_xformers,
        use_chunked_processing=args.chunked_processing,
        chunk_size=args.chunk_size,
    ).to(device)
    
    # å¯ç”¨xformersä¼˜åŒ–
    if args.enable_xformers:
        try:
            pipeline.enable_xformers_memory_efficient_attention()
            print("âœ… Enabled xformers memory efficient attention")
        except:
            print("âš ï¸ Xformers not available, using default attention")
    
    # å¯ç”¨DeepCacheä¼˜åŒ–
    if args.enable_deepcache:
        print("ğŸ’¾ Enabling DeepCache optimization...")
        helper = DeepCacheSDHelper(pipe=pipeline)
        
        # æ ¹æ®è§†é¢‘é•¿åº¦åŠ¨æ€è°ƒæ•´ç¼“å­˜å‚æ•°
        video_length = VideoProcessor.estimate_video_length(args.video_path)
        if video_length > 300:  # 5åˆ†é’Ÿä»¥ä¸Š
            cache_interval = 5
            cache_branch_id = 0
            print(f"â±ï¸ Long video ({video_length}s): cache_interval={cache_interval}")
        elif video_length > 60:  # 1-5åˆ†é’Ÿ
            cache_interval = 3
            cache_branch_id = 0
            print(f"â±ï¸ Medium video ({video_length}s): cache_interval={cache_interval}")
        else:  # 1åˆ†é’Ÿä»¥å†…
            cache_interval = 2
            cache_branch_id = 0
            print(f"â±ï¸ Short video ({video_length}s): cache_interval={cache_interval}")
        
        helper.set_params(
            cache_interval=cache_interval,
            cache_branch_id=cache_branch_id
        )
        helper.enable()
        print("âœ… DeepCache enabled")
    
    # è®¾ç½®éšæœºç§å­
    if args.seed != -1:
        set_seed(args.seed)
        print(f"ğŸ² Using fixed seed: {args.seed}")
    else:
        seed = torch.seed()
        print(f"ğŸ² Using random seed: {seed}")
    
    # æ¸…ç†å†…å­˜
    MemoryOptimizer.clear_memory()
    
    # æ‰§è¡Œæ¨ç†
    print("\n" + "="*50)
    print("ğŸ¬ Starting inference...")
    print("="*50)
    
    inference_start = time.time()
    
    # è°ƒç”¨pipeline
    pipeline(
        video_path=args.video_path,
        audio_path=args.audio_path,
        video_out_path=args.video_out_path,
        num_frames=config.data.num_frames,
        num_inference_steps=args.inference_steps,
        guidance_scale=args.guidance_scale,
        weight_dtype=dtype,
        width=config.data.resolution,
        height=config.data.resolution,
        mask_image_path=config.data.mask_image_path,
        temp_dir=args.temp_dir,
    )
    
    inference_time = time.time() - inference_start
    total_time = time.time() - start_time
    
    # è¾“å‡ºæ€§èƒ½ç»Ÿè®¡
    print("\n" + "="*50)
    print("ğŸ“Š Performance Statistics")
    print("="*50)
    print(f"ğŸ•’ Total time: {total_time:.2f}s")
    print(f"âš¡ Inference time: {inference_time:.2f}s")
    print(f"ğŸš€ Setup time: {total_time - inference_time:.2f}s")
    
    # å†…å­˜ä½¿ç”¨ç»Ÿè®¡
    if torch.cuda.is_available():
        mem_info = MemoryOptimizer.get_gpu_memory_info()
        print(f"ğŸ’¾ GPU Memory - Allocated: {mem_info['allocated']:.2f}GB")
        print(f"ğŸ’¾ GPU Memory - Cached: {mem_info['cached']:.2f}GB")
        print(f"ğŸ’¾ GPU Memory - Total: {mem_info['total']:.2f}GB")
    
    print(f"âœ… Inference completed! Output saved to: {args.video_out_path}")
    print("="*50)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Optimized LatentSync Inference")
    
    # å¿…éœ€å‚æ•°
    parser.add_argument("--unet_config_path", type=str, default="configs/unet.yaml")
    parser.add_argument("--inference_ckpt_path", type=str, required=True)
    parser.add_argument("--video_path", type=str, required=True)
    parser.add_argument("--audio_path", type=str, required=True)
    parser.add_argument("--video_out_path", type=str, required=True)
    
    # æ¨ç†å‚æ•°
    parser.add_argument("--inference_steps", type=int, default=12,
                       help="Number of denoising steps (default: 12, was 20)")
    parser.add_argument("--guidance_scale", type=float, default=1.2,
                       help="Classifier-free guidance scale (default: 1.2, was 1.0)")
    
    # ä¼˜åŒ–å‚æ•°
    parser.add_argument("--temp_dir", type=str, default="/tmp/latentsync")
    parser.add_argument("--seed", type=int, default=1247)
    parser.add_argument("--enable_deepcache", action="store_true", default=True,
                       help="Enable DeepCache optimization")
    parser.add_argument("--enable_xformers", action="store_true", default=True,
                       help="Enable xformers optimization")
    parser.add_argument("--compile_model", action="store_true", default=True,
                       help="Compile model with torch.compile (PyTorch 2.0+)")
    parser.add_argument("--chunked_processing", action="store_true", default=True,
                       help="Process video in chunks to save memory")
    parser.add_argument("--chunk_size", type=int, default=16,
                       help="Number of frames per chunk (default: 16)")
    parser.add_argument("--temporal_stride", type=int, default=1,
                       help="Process every nth frame (default: 1)")
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    os.makedirs(args.temp_dir, exist_ok=True)
    
    # åŠ è½½é…ç½®
    config = OmegaConf.load(args.unet_config_path)
    
    # è¿è¡Œä¼˜åŒ–æ¨ç†
    main(config, args)