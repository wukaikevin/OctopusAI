{
  "variables": {"work_dir": "/tmp/Llama-3.2-11B-Vision-Instruct"},
  "author": "LLM-Research",
  "inputs": [
    {
      "editable": true,
      "default_value": "请详细描述这张图片的内容。",
      "label": "提示词",
      "type": "text",
      "required": true,
      "field_name": "prompt"
    },
    {
      "editable": true,
      "default_value": "",
      "label": "输入图片",
      "type": "file",
      "required": true,
      "field_name": "image"
    },
    {
      "editable": true,
      "default_value": 200,
      "label": "生成长度",
      "type": "number",
      "required": false,
      "field_name": "max_new_tokens"
    }
  ],
  "batch": [{
    "env_name": "llama32_vision_env",
    "name": "模型推理",
    "commands": [
      "mkdir -p $(dirname {server_output})",
      "cd {work_dir}",
      "printf '%s' \"{prompt}\" > prompt.txt",
      "python -c \"\nimport os\nimport sys\n\nfrom PIL import Image\nfrom modelscope import MllamaForConditionalGeneration, AutoProcessor\nimport torch\n\nmodel_id = 'LLM-Research/Llama-3.2-11B-Vision-Instruct'\n\nprint('Loading model...')\nmodel = MllamaForConditionalGeneration.from_pretrained(\n    model_id,\n    device_map='auto'\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nprint('Reading input...')\nwith open('prompt.txt', 'r', encoding='utf-8') as f:\n    prompt_text = f.read()\n\nimage_path = '{image}'\nmax_tokens = int({max_new_tokens})\noutput_path = '{server_output}'\n\nimage = Image.open(image_path)\n\nmessages = [\n    {'role': 'user', 'content': [\n        {'type': 'image'},\n        {'type': 'text', 'text': prompt_text}\n    ]}\n]\ninput_text = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(\n    image,\n    input_text,\n    add_special_tokens=False,\n    return_tensors='pt'\n).to(model.device)\n\nprint('Generating response...')\noutput = model.generate(**inputs, max_new_tokens=max_tokens)\nresult_text = processor.decode(output[0])\n\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\nwith open(output_path, 'w', encoding='utf-8') as f:\n    f.write(result_text)\n\nprint('Done!')\n\""
    ]
  }],
  "description": "Llama-3.2-11B-Vision-Instruct是一个多模态大语言模型，能够根据输入的图像和文本提示生成详细的回答。适用于图像描述、文档问答、图表理解等场景。",
  "model_path": "/root/.cache/modelscope",
  "installs": [{
    "env_name": "llama32_vision_env",
    "cuda_version": "12.1",
    "python_version": "3.10",
    "commands": [
      "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121",
      "pip install \"modelscope[multi-modal]\" pillow",
      "mkdir -p {model_path}",
      "rm -rf {work_dir}",
      "mkdir -p {work_dir}"
    ]
  }],
  "uploads": [],
  "endpoint": "cat \"{server_output}\"",
  "server_output": "/tmp/Llama-3.2-11B-Vision-Instruct/output/result.txt",
  "name": "图像描述-Llama-3.2-11B-Vision-Instruct",
  "app_id": "LLM-Research-Llama-3.2-11B-Vision-Instruct",
  "email": "",
  "homepage": "https://www.modelscope.cn/models/LLM-Research/Llama-3.2-11B-Vision-Instruct"
}