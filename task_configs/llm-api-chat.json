{
  "app_id": "llm-api-chat",
  "name": "LLM大模型对话调用",
  "description": "基于标准OpenAI格式的LLM大模型API调用工具，支持通过HTTP请求与各类兼容OpenAI协议的大语言模型进行对话。支持流式输出和非流式输出两种模式，可选JSON格式响应。无需Authorization认证，直接调用指定API地址进行对话交互。适用于大模型测试、对话生成、文本处理等场景。",
  "author": "User",
  "homepage": "",
  "email": "",
  "inputs": [
    {
      "label": "提示词",
      "field_name": "prompt",
      "type": "textarea",
      "default_value": "请介绍一下人工智能的发展历程。",
      "editable": true,
      "required": true
    },
    {
      "label": "是否启用流式输出",
      "field_name": "stream",
      "type": "select",
      "default_value": "false",
      "editable": true,
      "required": true,
      "options": [
        { "label": "否（非流式，一次性返回完整结果）", "value": "false" },
        { "label": "是（流式输出，逐块返回内容）", "value": "true" }
      ]
    },
    {
      "label": "是否启用JSON格式输出",
      "field_name": "response_format",
      "type": "select",
      "default_value": "auto",
      "editable": true,
      "required": true,
      "options": [
        { "label": "自动（文本格式）", "value": "auto" },
        { "label": "强制JSON格式", "value": "json" }
      ]
    },
    {
      "label": "系统提示词（可选，用于设定AI角色或行为约束）",
      "field_name": "system_prompt",
      "type": "textarea",
      "default_value": "你是一个专业、友好、乐于助人的AI助手。",
      "editable": true,
      "required": false
    },
    {
      "label": "模型名称",
      "field_name": "model",
      "type": "text",
      "default_value": "glm-4.7",
      "editable": true,
      "required": false
    },
    {
      "label": "温度参数（0-2，值越高输出越随机，越低越确定）",
      "field_name": "temperature",
      "type": "number",
      "default_value": 0.7,
      "editable": true,
      "required": false
    },
    {
      "label": "最大生成Token数（控制输出长度）",
      "field_name": "max_tokens",
      "type": "number",
      "default_value": 2048,
      "editable": true,
      "required": false
    }
  ],
  "variables": {
    "work_dir": "$HOME/llm-api-chat"
  },
  "server_output_file": "/tmp/llm-api-chat-results/response.txt",
  "server_output_dir": "/tmp/llm-api-chat-results",
  "model_path": "",
  "installs": [
    {
      "env_name": "llm-api-chat-env",
      "python_version": "3.10",
      "cuda_version": "12.1",
      "commands": [
        "mkdir -p {work_dir}",
        "mkdir -p {server_output_dir}",
        "pip install requests"
      ]
    }
  ],
  "batch": [
    {
      "name": "调用LLM API",
      "env_name": "llm-api-chat-env",
      "commands": [
        "rm -rf {server_output_dir}/*",
        "mkdir -p {server_output_dir}",
        "cd {work_dir}",
        "cat > call_llm_api.py << 'EOF'",
        "import requests",
        "import json",
        "import sys",
        "",
        "# 强制禁用所有输出缓冲",
        "sys.stdout.reconfigure(line_buffering=True)",
        "",
        "# API配置",
        "API_URL = \"https://94402e3092d8e584-80.cn-south-nas-1.gpu-instance.ppinfra.com/api/paas/v4/chat/completions\"",
        "",
        "# 用户输入参数",
        "prompt = \"\"\"{prompt}\"\"\"",
        "system_prompt = \"\"\"{system_prompt}\"\"\"",
        "# 转换字符串为布尔值",
        "stream_str = \"{stream}\"",
        "stream = stream_str.lower() in ('true', '1', 'yes')",
        "response_format_type = \"{response_format}\"",
        "model_name = \"{model}\" if \"{model}\" else \"glm-4.7\"",
        "# 处理数值参数",
        "temperature = float(\"{temperature}\") if \"{temperature}\" else 0.7",
        "max_tokens = int(\"{max_tokens}\") if \"{max_tokens}\" else 2048",
        "",
        "# 构建请求头（不需要Authorization）",
        "headers = {",
        "    \"Content-Type\": \"application/json\"",
        "}",
        "",
        "# 构建请求体",
        "payload = {",
        "    \"model\": model_name,",
        "    \"messages\": [",
        "        {\"role\": \"system\", \"content\": system_prompt},",
        "        {\"role\": \"user\", \"content\": prompt}",
        "    ],",
        "    \"stream\": stream,",
        "    \"temperature\": temperature,",
        "    \"max_tokens\": max_tokens",
        "}",
        "",
        "# 如果启用JSON格式输出",
        "if response_format_type == \"json\":",
        "    payload[\"response_format\"] = {\"type\": \"json_object\"}",
        "",
        "print(f\"正在调用LLM API...模型: {model_name}\")",
        "print(\"-\" * 50)",
        "",
        "# 发送请求",
        "try:",
        "    response = requests.post(API_URL, headers=headers, json=payload, timeout=3000, stream=stream)",
        "    response.raise_for_status()",
        "    ",
        "    # 处理响应",
        "    if stream:",
        "        # 流式输出处理",
        "        full_content = \"\"",
        "        for line in response.iter_lines(decode_unicode=True, chunk_size=1):",
        "            if line:",
        "                if line.startswith('data: '):",
        "                    data = line[6:]",
        "                    if data == '[DONE]':",
        "                        break",
        "                    try:",
        "                        json_data = json.loads(data)",
        "                        if 'choices' in json_data and len(json_data['choices']) > 0:",
        "                            delta = json_data['choices'][0].get('delta', {})",
        "                            content = delta.get('content', '')",
        "                            if content:",
        "                                # 使用sys.stdout.write确保立即输出",
        "                                sys.stdout.write(content)",
        "                                sys.stdout.flush()",
        "                                full_content += content",
        "                    except json.JSONDecodeError:",
        "                        pass",
        "        print()  # 换行",
        "        output_content = full_content if full_content else \"（流式输出未获取到内容）\"",
        "    else:",
        "        # 非流式输出处理",
        "        result = response.json()",
        "        if 'choices' in result and len(result['choices']) > 0:",
        "            output_content = result['choices'][0]['message']['content']",
        "            print(output_content)",
        "        else:",
        "            output_content = json.dumps(result, ensure_ascii=False, indent=2)",
        "            print(output_content)",
        "    ",
        "    # 保存到文件",
        "    output_file = \"{server_output_file}\"",
        "    with open(output_file, 'w', encoding='utf-8') as f:",
        "        f.write(output_content)",
        "    ",
        "    print(\"-\" * 50)",
        "    print(f\"响应已保存到: {output_file}\")",
        "    ",
        "except requests.exceptions.RequestException as e:",
        "    error_msg = f\"API调用失败: {str(e)}\\n响应状态码: {e.response.status_code if hasattr(e, 'response') and e.response else 'N/A'}\\n响应内容: {e.response.text if hasattr(e, 'response') and e.response else 'N/A'}\"",
        "    print(error_msg, file=sys.stderr)",
        "    # 保存错误信息到文件",
        "    with open(\"{server_output_file}\", 'w', encoding='utf-8') as f:",
        "        f.write(error_msg)",
        "    sys.exit(1)",
        "except Exception as e:",
        "    error_msg = f\"发生错误: {type(e).__name__}: {str(e)}\"",
        "    print(error_msg, file=sys.stderr)",
        "    with open(\"{server_output_file}\", 'w', encoding='utf-8') as f:",
        "        f.write(error_msg)",
        "    sys.exit(1)",
        "EOF",
        "# 使用 stdbuf 和 python -u 确保完全无缓冲输出",
        "stdbuf -oL python -u call_llm_api.py"
      ]
    }
  ],
  "endpoint": "cat \"{server_output_file}\""
}
